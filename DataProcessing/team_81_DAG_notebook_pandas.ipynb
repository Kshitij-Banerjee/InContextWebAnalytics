{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import Packages",
   "id": "c609fc05bd9b3eaf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:29:44.638670Z",
     "start_time": "2024-11-14T12:29:43.800375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta"
   ],
   "id": "8cd7891dcbb559f7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import file",
   "id": "2c2db617bdb1141b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:30:52.744896Z",
     "start_time": "2024-11-14T12:29:44.640187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a DataFrame\n",
    "df = pd.read_csv('4f-lv-final-export.csv', sep=';', encoding='ISO-8859-1')\n",
    "\n",
    "# Convert 'time' column to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])"
   ],
   "id": "9bd2cd96f1f81b5f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Filter unnecessary columns",
   "id": "8a5f141e22f92f58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:30:52.749172Z",
     "start_time": "2024-11-14T12:30:52.745411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# columns = [\"token\", \"fingerprint\", \"userToken\", \"siteId\", \"userId\", \"url\", \"type\", \"group\",\n",
    "#            \"category\", \"action\", \"name\", \"time\", \"visitKey\", \"groupSource\"]\n",
    "# columns_to_keep = [\"token\", \"userToken\", \"userId\", \"url\", \"type\", \"category\", \"time\"]\n",
    "# may_be = [\"visitKey\", \"groupSource\"]\n",
    "\n",
    "# TODO: 1. Find start and end point of time\n",
    "# TODO: 2. Create id table for url. Use \"page_id\" (newly made one), \"url\", \"category\" as \"page_category\" STRING, \"type\" as \"page_type\" STRING\n",
    "# TODO: 3. Create node table with \"page_id\" as INT64, visits_last_7_days, unique_visitors_last_7_days, trend_percentage\n",
    "# TODO: 4. Create edge table with \"source_page_id\" as INT64, \"destination_page_id\" as INT64, \"transition_count\" as INT64,\n",
    "# TODO    \"avg_time_between_pages\" as FLOAT64, \"last_visit_timestamp TIMESTAMP\", \"avg_path_depth as FLOAT64\"\n",
    "\n",
    "# Filter columns with necessary information only\n",
    "def sort_and_filter_clicksream_df(df):\n",
    "    df_sorted = df.sort_values(by=['token', 'time'])\n",
    "    columns_to_keep = [\"token\", \"userToken\", \"userId\", \"url\", \"type\", \"category\", \"time\", \"visitKey\", \"groupSource\"]\n",
    "    return df_sorted[columns_to_keep]"
   ],
   "id": "f9894556e8a48492",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Create URL table (url_table) for storing page (node) information for each page_id\n",
    "Also create merge_url function for future"
   ],
   "id": "692e15e89e461720"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:30:52.758539Z",
     "start_time": "2024-11-14T12:30:52.749486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_url_table(df):\n",
    "    # Select the relevant columns and drop duplicates to get unique URLs\n",
    "    url_table = df[['url', 'category', 'type']].drop_duplicates()\n",
    "    \n",
    "    # Add a unique page_id for each unique URL\n",
    "    url_table = url_table.reset_index(drop=True)\n",
    "    url_table['page_id'] = url_table.index + 1  # Start page_id from 1\n",
    "    \n",
    "    # Rename columns as specified\n",
    "    url_table = url_table.rename(columns={\n",
    "        'category': 'page_category',\n",
    "        'type': 'page_type'\n",
    "    })\n",
    "    \n",
    "    # Reorder columns to match the required output\n",
    "    url_table = url_table[['page_id', 'url', 'page_category', 'page_type']]\n",
    "    \n",
    "    return url_table\n",
    "\n",
    "def replace_url_with_id(df, url_table):\n",
    "    # Merge df_filtered with url_table on 'url' to get the corresponding 'page_id'\n",
    "    df_merged = df.merge(url_table[['page_id', 'url']], on='url', how='left')\n",
    "    # Drop the original 'url' column and rename 'page_id' column\n",
    "    df_merged = df_merged.drop(columns=['url']).rename(columns={'page_id': 'page_id'})\n",
    "    return df_merged\n",
    "\n",
    "def merge_url(df, url_table):\n",
    "    df_merged = df.merge(url_table, on='page_id', how='left')\n",
    "    return df_merged"
   ],
   "id": "59d76924a577af8b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:30:52.764387Z",
     "start_time": "2024-11-14T12:30:52.759549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_edges_table(df):\n",
    "    # Sort by token and time for each user's navigation sequence\n",
    "    df_sorted = df.sort_values(by=['token', 'time'])\n",
    "    \n",
    "    # Shift data to get source and destination pages and times\n",
    "    df_sorted['next_page_id'] = df_sorted.groupby('token')['page_id'].shift(-1)\n",
    "    df_sorted['source_time'] = df_sorted['time']\n",
    "    df_sorted['destination_time'] = df_sorted.groupby('token')['time'].shift(-1)\n",
    "    \n",
    "    # Remove rows where next_page_id or destination_time is NaN (last page in each token sequence)\n",
    "    edges = df_sorted.dropna(subset=['next_page_id', 'destination_time'])\n",
    "    \n",
    "    # Rename and select relevant columns for edges\n",
    "    edges = edges.rename(columns={'page_id': 'source_page_id', 'next_page_id': 'destination_page_id'})\n",
    "    \n",
    "    # Add depth, which is the order of the visit for each token group\n",
    "    edges['depth'] = edges.groupby('token').cumcount() + 1\n",
    "    \n",
    "    # Calc time difference\n",
    "    edges['time_diff'] = (edges['destination_time'] - edges['source_time']).dt.total_seconds()\n",
    "    \n",
    "    # Ensure ids are in int\n",
    "    edges['source_page_id'] = edges['source_page_id'].astype(int)\n",
    "    edges['destination_page_id'] = edges['destination_page_id'].astype(int)\n",
    "    \n",
    "    # Select columns for the edge data\n",
    "    edge_data = edges[['token', 'source_page_id', 'destination_page_id', 'source_time', 'destination_time', 'time_diff', 'depth']]\n",
    "    \n",
    "    return edge_data\n"
   ],
   "id": "db5824a6b492586",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a function that export edges from set time frame then aggregate edges and calculate averages\n",
   "id": "790f7f67a5a30755"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:30:52.775209Z",
     "start_time": "2024-11-14T12:30:52.765911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_edges_table_export(edges_df, time, duration):\n",
    "    \"\"\"\n",
    "    Generate an aggregated export edges table from whole edges table based on unique edges within a specified time range.\n",
    "    Parameters:\n",
    "        edges_df (DataFrame): The DataFrame containing edge data.\n",
    "        time (str): The start date for filtering in 'YYYY-MM-DD' format.\n",
    "        duration (int): The number of days from the start time to determine the end time.\n",
    "    Returns:\n",
    "        DataFrame: Aggregated edges table with the required columns.\n",
    "    \"\"\"\n",
    "    time = pd.to_datetime(time)\n",
    "    if duration >= 0: # Determine the start and end of the time range based on duration\n",
    "        start_time = time\n",
    "        end_time = time + timedelta(days=duration)\n",
    "    else:\n",
    "        start_time = time + timedelta(days=duration)\n",
    "        end_time = time\n",
    "    \n",
    "    # Filter edges within the specified time range\n",
    "    edges_filtered = edges_df[(edges_df['source_time'] >= start_time) & (edges_df['source_time'] <= end_time)]\n",
    "    \n",
    "    # Aggregate data by unique edges (source_page_id, destination_page_id)\n",
    "    edges_table_export = edges_filtered.groupby(['source_page_id', 'destination_page_id']).agg(\n",
    "        transition_count=('depth', 'count'),\n",
    "        avg_time_between_pages=('time_diff', 'mean'),\n",
    "        last_visit_timestamp=('destination_time', 'max'),\n",
    "        avg_path_depth=('depth', 'mean'),\n",
    "        unique_token_count=('token', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Ensure correct data types for each columns\n",
    "    edges_table_export['source_page_id'] = edges_table_export['source_page_id'].astype('int64')\n",
    "    edges_table_export['destination_page_id'] = edges_table_export['destination_page_id'].astype('int64')\n",
    "    edges_table_export['transition_count'] = edges_table_export['transition_count'].astype('int64')\n",
    "    edges_table_export['avg_time_between_pages'] = edges_table_export['avg_time_between_pages'].astype('float64')\n",
    "    edges_table_export['avg_path_depth'] = edges_table_export['avg_path_depth'].astype('float64')\n",
    "    edges_table_export['unique_token_count'] = edges_table_export['unique_token_count'].astype('int64')\n",
    "\n",
    "    return edges_table_export\n",
    "\n",
    "def create_nodes_table_export(edges_df, time, n_days):\n",
    "    # TODO: may need to change outcome of the trend result. Note that it is replacing value with 99999 if 0 is detected for prev result\n",
    "    \"\"\"\n",
    "    Generate the nodes table export based on edge data.\n",
    "    Parameters:\n",
    "        edges_df (DataFrame): The DataFrame containing edge data.\n",
    "        time (str): The reference date for filtering in 'YYYY-MM-DD' format.\n",
    "        n (int): Number of days for the last period and the previous period comparison.\n",
    "    Returns:\n",
    "        DataFrame: Nodes table with the required columns.\n",
    "    \"\"\"\n",
    "    time = pd.to_datetime(time)\n",
    "    last_period_start = time - timedelta(days=n_days)\n",
    "    prev_period_start = time - timedelta(days=2 * n_days)\n",
    "    \n",
    "    # Get edges within the last n days and previous n days\n",
    "    last_period_edges = create_edges_table_export(edges_df, last_period_start, n_days)\n",
    "    prev_period_edges = create_edges_table_export(edges_df, prev_period_start, n_days)\n",
    "    \n",
    "    # Aggregate visits and unique visitors (sum of unique_token_count) in the last n days\n",
    "    last_period_visits = last_period_edges.groupby('destination_page_id').agg(\n",
    "        visits_last_n_days=('transition_count', 'sum'),\n",
    "        unique_visitors_last_n_days=('unique_token_count', 'sum')  # Sum unique token counts for each destination page\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Aggregate visits in the previous n days\n",
    "    prev_period_visits = prev_period_edges.groupby('destination_page_id').agg(\n",
    "        prev_visits=('transition_count', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Merge data for trend calculation\n",
    "    nodes_table_export = pd.merge(last_period_visits, prev_period_visits, on='destination_page_id', how='left').fillna(0)\n",
    "    \n",
    "    # Calculate trend percentage with the condition for 99999 when prev_visits is 0\n",
    "    nodes_table_export['trend_percentage'] = nodes_table_export.apply(\n",
    "        lambda row: 99999 if row['prev_visits'] == 0 else\n",
    "        ((row['visits_last_n_days'] - row['prev_visits']) / row['prev_visits']) * 100, axis=1\n",
    "    )\n",
    "    \n",
    "    # Rename destination_page_id to page_id for consistency\n",
    "    nodes_table_export = nodes_table_export.rename(columns={'destination_page_id': 'page_id'})\n",
    "    \n",
    "    # Ensure correct data types\n",
    "    nodes_table_export['page_id'] = nodes_table_export['page_id'].astype('int64')\n",
    "    nodes_table_export['visits_last_' + str(n_days) + '_days'] = nodes_table_export['visits_last_n_days'].astype('int64')\n",
    "    nodes_table_export['unique_visitors_last_' + str(n_days) + '_days'] = nodes_table_export['unique_visitors_last_n_days'].astype('int64')\n",
    "    nodes_table_export['trend_percentage'] = nodes_table_export['trend_percentage'].astype('float64')\n",
    "    \n",
    "    nodes_table_export = nodes_table_export.drop(columns=['visits_last_n_days', 'unique_visitors_last_n_days'])\n",
    "\n",
    "    return nodes_table_export\n",
    "\n",
    "\n",
    "def export_edges_and_nodes(edges_df, url_table, time, days = 7):\n",
    "    edges_export = create_edges_table_export(edges_df, time, days)\n",
    "    nodes_export = create_nodes_table_export(edges_df, time, days)\n",
    "    \n",
    "    # Drop unnecessary tables\n",
    "    edges_export = edges_export.drop(columns=['unique_token_count'])\n",
    "    nodes_export = nodes_export.drop(columns=['prev_visits'])\n",
    "    \n",
    "    nodes_export_with_url = merge_url(nodes_export, url_table)\n",
    "    \n",
    "    return edges_export, nodes_export, url_table, nodes_export_with_url"
   ],
   "id": "5ddf65a7e275c932",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check functions",
   "id": "9b41f5338b36821"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:33:45.175965Z",
     "start_time": "2024-11-14T12:30:52.775727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_filtered = sort_and_filter_clicksream_df(df)\n",
    "url_table = create_url_table(df_filtered)\n",
    "# (optional) check times\n",
    "# start_time = df_filtered['time'].min()\n",
    "# end_time = df_filtered['time'].max()\n",
    "# print(\"Start Time:\", start_time)\n",
    "# print(\"End Time:\", end_time)\n",
    "df_filtered = replace_url_with_id(df_filtered, url_table)\n",
    "edge_data = create_edges_table(df_filtered)\n",
    "edges_export, nodes_export, url_table, nodes_export_with_url = export_edges_and_nodes(edge_data, url_table, \"2023-04-15\", days = 7)"
   ],
   "id": "a61ae8f9efe44421",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:33:45.234927Z",
     "start_time": "2024-11-14T12:33:45.184645Z"
    }
   },
   "cell_type": "code",
   "source": "print(edges_export)",
   "id": "164487d1a27caab2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        source_page_id  destination_page_id  transition_count  \\\n",
      "0                    2                 3349               159   \n",
      "1                    3                  757               366   \n",
      "2                    7                    7                12   \n",
      "3                    7                   35                18   \n",
      "4                    7                   36                 1   \n",
      "...                ...                  ...               ...   \n",
      "111695          488275                  276                 1   \n",
      "111696          488275                  999                 1   \n",
      "111697          488275                 1349                 1   \n",
      "111698          488275                 7055                11   \n",
      "111699          488275                55156                 1   \n",
      "\n",
      "        avg_time_between_pages last_visit_timestamp  avg_path_depth  \n",
      "0                     0.000000  2023-04-21 06:23:05       13.220126  \n",
      "1                     0.000000  2023-04-21 21:13:56       35.103825  \n",
      "2                    15.916667  2023-04-21 13:21:21      169.666667  \n",
      "3                    28.277778  2023-04-21 15:34:19      126.111111  \n",
      "4                    16.000000  2023-04-17 14:44:22       22.000000  \n",
      "...                        ...                  ...             ...  \n",
      "111695               19.000000  2023-04-16 15:52:46      216.000000  \n",
      "111696                8.000000  2023-04-18 19:52:17      274.000000  \n",
      "111697               61.000000  2023-04-16 15:51:05      178.000000  \n",
      "111698                4.363636  2023-04-18 19:52:09      184.727273  \n",
      "111699               17.000000  2023-04-18 09:14:58       21.000000  \n",
      "\n",
      "[111700 rows x 6 columns]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:33:45.244109Z",
     "start_time": "2024-11-14T12:33:45.235939Z"
    }
   },
   "cell_type": "code",
   "source": "print(nodes_export_with_url)",
   "id": "26a49bf1dbff7e36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       page_id  trend_percentage  visits_last_7_days  \\\n",
      "0            2        -46.969697                 105   \n",
      "1            3        -75.484872                1264   \n",
      "2            7         19.354839                 111   \n",
      "3            8        -77.813318                 763   \n",
      "4           10        -97.981497                  24   \n",
      "...        ...               ...                 ...   \n",
      "42498   488217        -33.333333                   6   \n",
      "42499   488223      99999.000000                  21   \n",
      "42500   488235        480.000000                  29   \n",
      "42501   488240        -33.333333                   2   \n",
      "42502   488275        -25.000000                   9   \n",
      "\n",
      "       unique_visitors_last_7_days  \\\n",
      "0                               46   \n",
      "1                              751   \n",
      "2                              110   \n",
      "3                              409   \n",
      "4                               18   \n",
      "...                            ...   \n",
      "42498                            1   \n",
      "42499                            4   \n",
      "42500                            6   \n",
      "42501                            1   \n",
      "42502                            2   \n",
      "\n",
      "                                                     url page_category  \\\n",
      "0      https://4fstore.lv/sieviesu-dunu-bezrocis-ar-s...           NaN   \n",
      "1      https://4fstore.lv/kategorijas/vasaras-kolekci...           NaN   \n",
      "2                    https://4fstore.lv/landing/par-mums           NaN   \n",
      "3      https://4fstore.lv/kategorijas/vasaras-kolekci...           NaN   \n",
      "4      https://4fstore.lv/sievietes/apgerbs/bikses-un...           NaN   \n",
      "...                                                  ...           ...   \n",
      "42498  https://4fstore.lv/sieviesu-termoaktiva-mateta...   ProductPage   \n",
      "42499  https://4fstore.lv/atpogajama-viriesu-sporta-j...   ProductPage   \n",
      "42500  https://4fstore.lv/meitenu-sandales-4fjs23fsan...          Cart   \n",
      "42501  https://4fstore.lv/zenu-termoaktiva-mateta-apa...   ProductPage   \n",
      "42502  https://4fstore.lv/atpogajama-meitenu-sporta-j...          Cart   \n",
      "\n",
      "      page_type  \n",
      "0       product  \n",
      "1       listing  \n",
      "2       landing  \n",
      "3       listing  \n",
      "4       listing  \n",
      "...         ...  \n",
      "42498   product  \n",
      "42499   product  \n",
      "42500   product  \n",
      "42501   product  \n",
      "42502   product  \n",
      "\n",
      "[42503 rows x 7 columns]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:38:27.696376Z",
     "start_time": "2024-11-14T12:33:45.245387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "edge_data.to_csv('edge_data.csv', index=False)\n",
    "edges_export.to_csv('edges_export.csv', index=False)\n",
    "nodes_export.to_csv('nodes_export.csv', index=False)\n",
    "url_table.to_csv('url_table.csv', index=False)\n",
    "nodes_export_with_url.to_csv('nodes_export_with_url.csv', index=False)"
   ],
   "id": "d7f37b0dbf92771f",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
